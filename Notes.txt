Microservices Architecture
A variant of the service-oriented architecture (SOA) structural style arranges an application as a collection of loosely coupled services. In a microservices 
architecture, services are fine-grained and the protocols are lightweight.

Monolithic Architecture
• Built as a single unit
• Deployed as a single unit
• Duplicated on each server
• Ex: 3-tier apps

Microservices
• Segregates functionality into smaller separate services each with a single responsibility 
• Scales out by deploying each service independently
• Loosely coupled
• Enable autonomous development by different teams, languages and platforms
• Can be written by smaller teams
• Each microservice can own it's own data/database

Microservices - Benefits
• Improved fault isolation
• Eliminate vendor or technology lock-in
• Ease of understanding
• Smaller and faster deployments
• Scalability

Cloud Native Foundation
Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid
clouds. Containers, service meshes, microservices, immutable infrastructure and declarative APIs exemplify this approach.
These techniques enable loosely coupled systems that are resilient, manageable and observable. Combined with robust automation, they allow engineers to make 
high-impact changes frequently and predictably with minimal toil.
The Cloud Native Computing Foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendor neutral
projects. We democratize state-of-the-art patterns to make these innovations accessible for everyone.

Speed and Agility
• Among many things, cloud native is about speed and agility
• Users want...
	• Instantaneous responsiveness
	• Up-to-the-minute features
	• No downtime
• The business wants...
	• Accelerated innovation
	• Rapid releases of features to meet disruption from competitors
	• Increased confidence - stability/performance

What is a container
A unit of software/deployment. It contains everything needed for code to run (code, runtime, system tools, system libraries).

-----------------------------------------------------------------------------------------------------------------------------

Why containers?
• Move faster by deploying smaller units
• Use fewer resources
• Fit more into the same host
• Faster automation
• Portability
• Isolation

VM vs Containers

Virtual Machine
• Large footprint
• Slow to boot
• Ideal for long running tasks

Container
• Lightweight
• Quick to start (it does not have to boot)
• Portable
• Ideal for short lived tasks

Density

Containers are made of layers
(Application
Customizations
Base OS
)


Container registry

• Centralized container
repository
• Think GitHub for containers
• Docker Hub
	• https://hub.docker.com/
• Cloud Providers
	• AWS, Azure, Google Cloud


Orchestrator
• Manage
	• Infrastructure
	• Containers
	• Deployment
	• Scaling
	• Failover
	• Health monitoring
	• App upgrades, Zero-Downtime deployments
• Install your own
	• Kubernetes, Swarm, Service Fabric
• Orchestrators as a service
	• Azure Kubernetes Service, Service Fabric

-----------------------------------------------------------------------------------------------------------------------------

Docker

• An open source container runtime
• Mac, Windows & Linux support
• Command line tool
• "Dockerfile" file format for building container images
• Windows let your run Windows and Linux containers

-----------------------------------------------------------------------------------------------------------------------------

Docker CLI Management Commands-

docker info : Display system information
docker version : Display the system's version
docker login : Log in to a Docker registry

-----------------------------------------------------------------------------------------------------------------------------

Docker CLI Running and stopping commands-

docker pull [imageName] : Pull an image from a registry
docker run [imageName] : Run containers
docker run —d [imageName] : Detached mode
docker start [containerName] : Start stopped containers
docker ps : List running containers
docker ps -a : List running and stopped containers
docker stop [containerName] : Stop containers
docker kill [containerName] : Kill containers
docker image inspect [imageName] : Get image info

-----------------------------------------------------------------------------------------------------------------------------

Docker CLI Limits-

docker run --memory= "256m" nginx : Max memory
docker run --cpus=" . 5" nginx : Max CPU

-----------------------------------------------------------------------------------------------------------------------------
RUNNING A CONTAINER-

#pull and run an nginx server
docker run --publish 80:80 --name webserver nginx

#list the running containers
docker ps

#stop the container
docker stop webserver

#remove the container
docker rm webserver

-----------------------------------------------------------------------------------------------------------------------------

Docker CLI Attach shell-

docker run -it /bin/bash : Attach shell
docker run -it --microsoft/powershell:nanoserver pwsh.exe : Attach powershell
docker container exec -it [containerName] -- bash : Attach to running container

-----------------------------------------------------------------------------------------------------------------------------

Docker CLI Cleaning up-

docker rm [containerName] : Removes stopped containers
docker rm $(docker ps -a -q : Removes all stopped containers
docker images : Lists images
docker rmi [imageName] : Deletes the image
docker system prune -a ; Removes all images not in use by any containers

-----------------------------------------------------------------------------------------------------------------------------

Docker CLI building Containers-

docker build -t [name:tag] . -> Builds an image using dockerfile located in the same folder
docker build -t [name:tag] -f [filename] -> Builds an image using dockerfile located in a different folder
docker tag [imageName] [name:tag] -> Tag an existing image

-----------------------------------------------------------------------------------------------------------------------------

Containers are ephemerous and stateless-

• You usually don't store data in containers
• Non-persistent data
	• Locally on a writable layer
	• It's the default, just write to the filesystem
	• When containers are destroyed, so the data inside them
• Persistent data
	• Stored inside the container in a volume
	• A volume is mapped to logical folder

-----------------------------------------------------------------------------------------------------------------------------

Creating volumes-

docker create volume [volumeName] -> Creates a new volume
docker volume ls -> Lists the volume
docker volume inspect [volumeName] -> Display the volume info
docker volume rm [volumeName] -> Deletes a volume
docker volume prune -> Deletes all volume not mounted

-----------------------------------------------------------------------------------------------------------------------------

Mapping a volume-

# create a volume
docker volume create myvol

# inspect the volume
docker volume inspect myvol

# list the volumes
docker volume ls

# run a container with a volume
docker run -d --name devtest -v myvol:/app nginx:latest

#connect to the instance
docker exec -it voltest bash

#create a file in the volume using nano
apt-get update
apt-get install nano

# create file in app folder
cd app
nano test.txt

# Type something, save the file and exit Nano using:
ctrl+o
ctrl+x

# Detach from instance 
exit

# Stop and remove the container
docker stop voltest

# Stop and remove container
docker stop voltest
docker rm voltest

# Run again and see if the file exists or not
docker run -d --name voltest -v myvol:/app nginx:latest
docker exec -it voltest bash
cd app
cat test.txt

# Cleanup
docker volume rm myvol
docker stop voltest
docker rm voltest

-----------------------------------------------------------------------------------------------------------------------------

Mapping to a local folder-

# run a container with a volume
docker run -d --name devtest -v d:/test:/app nginx:latest

# inspect the container
docker inspect devtest

-----------------------------------------------------------------------------------------------------------------------------

YAML

• YAML: YAML Ain't Markup Language
• Human friendly data serialization standard
• Used by Docker-Compose and Kubernetes

-----------------------------------------------------------------------------------------------------------------------------

Docker Compose Concepts-

1. Multi containers apps: 
	• Define and run multi-containers applications
	• Define using YAML files
	• Run using the docker CLI with the compose plugin
	• Docker compose
	• Compose Specs
	• https://compose-spec.io 

2.Compose V2
	• GA at DockerCon Live 2022
	• Incorporates the docker compose command into the Docker CLI
		• You type docker compose instead of docker compose
		• Drop-in replacement for docker compose
		• docker compose commands maps directly to the docker compose ones
	• Installed with Docker Desktop
		• Linux: apt-get install docker compose-plugin
	• Written in Go
		• docker compose is written in Python
	• In summary, it's simply a faster version of the good old docker compose tool that is shipped as a plugin
	  instead of a Python app

	Docker Compose Use Cases-
	• Workloads that don't require a full orchestrator
	• Development and tests
	• Use of a service that can run Docker Compose files
		• Azure App Service
		• AWS ECS
		• Virtual machines

* Restart policy:
	• no
		• The default restart policy
		• Does not restart a container under any circumstances
	• always
		• Alwaya restarts the container until its removal
	• on-failure
		• Restart a container if exit code indicates an error
	• unless-stopped
		• Restart a container irrespective of the exit code but will stop restarting when the service is stopped 
		  or removed


-----------------------------------------------------------------------------------------------------------------------------

Docker Compose Commands-

docker compose build : Build the images
docker compose start : Start the containers
docker compose stop : Stop the containers
docker compose up -d : Build and start
docker compose ps : List what's running
docker compose rm : Remove from memory
docker compose down : Stop and remove
docker compose -p test down
docker compose logs : Get the logs
docker compose exec [container] bash : Run a command in a container
docker compose logs -f web-fe : Look at the db container logs

Compose V2 new commands-

docker compose --project-name test1 up -d : Run an instance as a project
docker compose -p test2 up -d : Shortcut
docker compose ls : List running projects
docker compose cp [containerID]:[SRC_PATH] [DEST_PATH] : Copy files from the container
docker compose cp [SRC_PATH] [containerID]:[DEST_PATH] : Copy files to the container

-----------------------------------------------------------------------------------------------------------------------------

Container Registeries-

• Central repository for container images.
• They can be private or public.
• Default is docker hub (hub.docker.com)
• Microsoft -
	• Azure Container registry
	• Microsoft Container registry (public images)
• Amazon elastic container registry
• Google container registry	


* PUSH AND PULL IMAGES TO DOCKER HUB:

docker build -t <YourRegistryName>/express:v1 . : build the image
docker push <YourRegistryName>/express:v1 : Push the image
docker rmi <YourRegistryName>/express:v1 : Remove the image
docker pull <YourRegistryName>/express:v1 : Pull the image

CREATE 2nd VERSION:
docker build -t <YourRegistryName>/express:v2 . : Build version 2 image
docker push <YourRegistryName>/express:v2 : Push the image

CLEANUP
docker rmi <YourRegistryName>/express:v1
docker rmi <YourRegistryName>/express:v2

-----------------------------------------------------------------------------------------------------------------------------

KUBERNETES (K8s):

• Originated from Google
• vl.O released on July 2015
• 3rd gen container scheduler from Google
	• Previous Google internal projects: Borg and Omega
• Donated to the Cloud Native Computing Foundation (CNCF)
	• www.cncf.io

• K8s is the leading container orchestration tool
• Designed as a loosely coupled collection of components centered around deploying, maintaining and scaling workloads
• Vendor-neutral
	• Runs on all cloud providers
• Backed by a huge community

What K8s can do -
• Service discovery and load balancing
• Storage orchestration
	• Local or Cloud based
• Automated rollouts and rollbacks
• Self-healing
• Secret and configuration management
• Use the same API across on-premise and every cloud providers

What K8s can't do -
• Does not deploy source code
• Does not build your application
• Does not provide application-level services
	• Message buses, databases, caches, etc

Architecture-
Container will run in a pod. A pod runs in a node. All the nodes form a cluster.
											CLUSTER
											Node
											pod

Running Kubernetes locally-
Local K8s
• Requires virtualization
	• Docker Desktop
	• MicroK8s
	• Minikube
• Runs over Docker Desktop
	• Kind

• Limited to 1 node
	• Docker Desktop
• Multiple nodes
	• MicroK8s
	• Kind
	• Minikube

(WINDOWS)
• Docker Desktop is currently the only way to run both Linux and Windows containers
• Docker Desktop can run on Hyper-V or WSL 2 (Win 10 2004)
• If Hyper-V is enabled, you can't run another hypervisor at the same time
• You can install Minikube on Hyper-V or VirtualBox

Minikube
• Does not require Docker Desktop
• Installs on Linux, macOS and Windows
	• https://kubernetes.io/docs/tasks/tools/install-minikube/
• An Hypervisor like VirtualBox is required

Kind
• Kubernetes IN Docker
	• https://kind.sigs.k8s.io/docs/user/quick-start/
• Runs on macOS, Linux and Windows
• Only requires Docker installed
	• No need for another VM installation
	• Installs the nodes as containers
• Windows installation via Chocolatey
	• choco install kind
• macOS installation via Brew
	• brew install kind

• Multi nodes
• High Availability Control Plane
• Define in YAML
• Useful if you need to test node affinity

K8s COMMANDS-
kubectl cluster-info : for testing local kubernetes installation.

K8s CLI and Context :
 	K8s API-

		kube-controller-manager
		cloud-controller-manager
		kube-apiserver            ----------------> The API server exposes REST API
			|	kube scheduler
			V
		   etcd

	CLI-
		• kubectl
		• Communicates with the apiserver
		• Configuration stored locally
			• ${HOME}/.kube/config
			• C:\Users\{USER}\.kube\config

	K8s Context
		• A context is a group of access parameters to a K8s cluster
		• Contains a Kubernetes cluster, a user, and a namespace
		• The current context is the cluster that is currently the default for kubectl
		• All kubectl commands run against that cluster
	
	kubectl Context Command -
		kubectl config current-context : Get the current context
		kubectl config get-contexts : List all context
		kubectl config use-context [contextName] : Set the current context
		kubectl config delete-context [contextName] : Delete a context from the config file
	
	Kubectx — Quickly switch context
		• Instead of typing
			• kubectl config use-context minikube
		• Simply type
			• kubectx [contextName]
		• Windows
			• choco install kubectx-ps
		• macOS
			• brew install kubectx
		• Ubuntu
			• sudo apt install kubectx
		• https://github.com/ahmetb/kubectx

-----------------------------------------------------------------------------------------------------------------------------

DECLARATIVE vs IMPERATIVE WAY OF CREATING RESOURCES IN KUBERNETES-

• Imperative
	• Using kubectl commands, issue a series of commands to create resources
	• Great for learning, testing and troubleshooting
	• It's like code
• Declarative
	• Using kubectl and YAML manifests defining the resources that you need
	• Reproducible, repeatable
	• Can be saved in source control
	• It's like data that can be parsed and modified

YAML file-
• Root level required properties
	• apiVersion
		• Api version of the object
	• kind
		• type of object
	• metadata.name
		• unique name for the object
	• metadata.namespace
		• scoped environment name (will default to current)
	• spec
		• object specifications or desired state
• Create an object using YAML
	• kubectl create -f [YAML file]

Commands-
kubectl create deployment mynginx1 --image=nginx : Imperative
kubectl create -f deploy-example.yaml : Declarative
kubectl delete deployment mynginx1 : Cleanup
kubectl delete deploy mynginx2 : Cleanup (short)
kubectl get deploy : to see all the deployments

-----------------------------------------------------------------------------------------------------------------------------

Namespaces-

• Allow to group resources
	Ex: Dev, Test, Prod
• K8s creates a default workspace
• Objects in one namespace can access objects in a different one
	• Ex: objectname.prod.svc.cluster.local
• Deleting a namespace will delete all its child objects

Namespace commands-
kubectl get namespace : List all namespaces
kubectl get ns : Shortcut
kubectl config set-context --current --namespace=[namespaceName] : Set the current context to use a namespace
kubectl create ns [namespaceName] : Create a namespace 
kubectl delete ns [namespaceName] : Delete a namespace
kubectl get pods --all-namespaces : List all pods in all namespaces
kubectl get pods : Display name of the current pod
kubectl get pods --namespace=kube-system : List all pods in a particular namespace(here kube-system)
kubectl get pods -n kube-system : Shortcut

-----------------------------------------------------------------------------------------------------------------------------

MASTER NODE-

• A nodes are physical or virtual machine. Together they form a cluster.
• Master node is also called Control plane.
• K8s services and controllers are located in control plane. They are also called master components.
• We usually don't the containers in the master node.

		kube-controller-manager
		cloud-controller-manager
		kube-apiserver      		-------> The API server is the only component communicating with the etcd
			|	kube scheduler
			V
		   etcd						--------> Key value datastore for cluster state data.


kube-apiserver- 

• REST interface
• Save state to the datastore (etcd)
• All clients interact with it, never directly to the datastore


etcd-

• Act as the cluster datastore for storing state
• key-value store
• Not a database or a datastore for applications to use
• The single source of truth


kube-control-manager -

• The controller of controllers!
• It runs controllers
	• Node controller
	• Replication controller
	• Endpoints controller
	• Service account & Token controllers


cloud-control-manager-

• Interact with the cloud providers controllers
	• Node
		• For checking the cloud provider to determine if a node has been deleted in the cloud after it stops 
		responding
	• Route
		• For setting up routes in the underlying cloud infrastructure
	• Service
		• For creating, updating and deleting cloud provider load balancers
	• Volume
		• For creating, attaching, and mounting volumes, and interacting with the cloud provider to
		 orchestrate volumes


kube-scheduler -

• Watches newly created pods that have no node assigned, and selects a node for them to run on
• Factors taken into account for scheduling decisions include
	• Individual and collective resource requirements
	• Hardware/software/policy constraints
	• Affinity and anti-affinity specifications
	• Data locality


Addons (additional functionalities in k8s cluster) -
• DNS
• Web Ul (dashboard)
• Cluster-level logging
• Container resource monitoring

-----------------------------------------------------------------------------------------------------------------------------

WORKER NODES- 

• The nodes running the container are called worker nodes.
• When worker nodes are added to the cluster, some k8s services are installed to the cluster automatically.
• container runtime, kubelet, kube-proxy...these are the services necessary to run pods and they are managed by 
   the master components on the master node.

kubelet
• Manage the pods lifecycle
• Ensures that the containers described in the Pod specs are running and healthy

kube-proxy
• A network proxy
• Manages network rules on nodes. All network traffic goes through the kube-proxy.


Container runtime-

• K8s supports several container runtimes
• Must implement the Kubernetes Container Runtime Interface
	• Moby
	• Containerd
	• Cri-O
	• Rkt
	• Kata
	• Virtlet


Container runtime - K8s V 1.19+  -
• Docker images run as is. It's business as usual!
• What's changed is what you can do inside the cluster
• You can no longer access the Docker engine inside the cluster
• Docker commands won't run if you ssh into a node
• Use crictl instead
(in previous version, container runtime was installed using an interface called dockershim(shim)).

Nodes pool
• A node pool is a group virtual machines, all with the same size
• A cluster can have multiple node pools
	• These pools can host different sizes ofVMs
	• Each pool can be autoscaled independently from the other pools
• Docker container is limited to 1 node.

Commands -
kubectl get nodes : List all installed nodes
kubectl describe node : To get some info about the node

-----------------------------------------------------------------------------------------------------------------------------

PODS-
• Atomic unit of the smallest unit of work of K8s
• Encapsulates an application's container
• Represents a unit of deployment
• Pods can run one or multiple containers
• Containers within a pod share
	• IP address space, mounted volumes
• Containers within a pod can communicate via
	• Localhost, IPC
• Pods are ephemeral
• Deploying a pod is an atomic operation, it succeed or not
• If a pod fails, it is replaced with a new one with a shiny new IP address
• You don't update a pod, you replace it with an updated version
• You scale by adding more pods, not more containers in a pod

* A node can run many pods and a pod can run one or more containers
* If a pod runs multiple containers then there is usually one that is the main worker where your application 
	logic is located. Others are the helper/sidecar container that provide services to the main worker.

POD LIFECYCLE-
• Pod Creation-
	When you issue a kube CTL create command to deploy a pod in your cluster, the CLI sends information to the
	API server and information will be written into etcd. 
	The scheduler will wash this type of information and find one where to schedule the Pod and write the 
	information in etcd. 
	The Kubelet running on the Node will watch for that information and issue a command to create an instance
	of container inside a Pod and finally the status will be written in etcd. 

	One thing to notice that each time an operation is taking place inside the cluster, the state is written in etcd. 
	So, etcd is the single source of truth in the cluster.

• Pod Deletion-
	When you issue a kube CTL delete command to delete a pod from your cluster, the CLI sends information to the API 
	server. That information will be written in etcd. Notice that the grace period of  30 seconds will be added. 
	The kubelet picks the information and sends a terminate signal to the container. If the container hangs it is killed 
	after the 32nd grace period and finally the state is stored in etcd. 

• Pod State- 
	It will gice you a high level summary of where the pod is in its lifecycle
	• Pending
		• Accepted but not yet created
	• Running
		• Bound to a node
	• Succeeded
		• Exited with status O
	• Failed
		• All containers exit and at least one exited with non-zero status
	• Unknow
		• Communication issues with the pod
	• CrashLoopBackOff
		• Started, crashed, started again, and then crashed again

kubectl- Pod commands -
kubectl create -f [pod-definition.yml] : Create pod
kubectl run [podname] --image=busybox -- /bin/sh -c "sleep 3600" : Run a pod
kubectl get pods : List the running pods
kubectl get pods -o wide : Same but with more info
kubectl describe pod [podname] : Show pod info
kubectl get pod [podname] -o yaml > file.yaml : Extract the pod definition in YAML and save it to a file
kubectl exec -it [podname] -- sh : Interactive mode
kubectl delete —f [pod-definition.yml] : Delete a pod
kubectl delete pod [podname] : Same using pod's name

###################################################
// Running BusyBox by attaching bash to our terminal:

	knubectl run mybox --image=busybox -it -- /bin/sh
	ls
	echo -n 'A Secret' | base64
	exit

//clean up 
	kubectl delete pod mybox --wait=false  
OR
	kubectl delete pod mybox --grace-period=0 --force 


// creating pod using declarative way

 	kubectl create -f myapp.yaml
	//get some info
	kubectl get pods -o wide
	kubectl describe pod myapp-pod
	//attach our terminal
	kubectl exec -it myapp-pod -- bash
	//Print the DBCON environment variable that was set in the YAML file:
	echo $DBCON
	//detach the instance
	exit
	//Cleanup
	kubectl delete -f myapp.yaml

###################################################

INIT CONTAINERS:

• Initialize a pod before an application container runs. This prevents you from cluttering the logic/structure
	of your application and good for application that contains dependencies. 
• Init container's job is to validate the db is running. This keeps the infrastructure code out of the main logic.
• Init containers always run to completion.
• Each container must complete successfully before the next one starts.
• If it fails, the kubelet repeatedly restarts it until it succeeds.
	• Unless it's policy is set to never.
• Probes are not supported.
	• livenessProbe, readinessProbe, or startupProbe


// Initializing nginx using init containers.

	// Create deployment
	kubectl apply -f myapp.yaml
	kubectl get pods

	//Connect to nginx container
	kubectl exec -it init-demo -- /bin/bash
	//Hit the default webpage
	curl localhost
	exit

	//Cleanup
	kubectl delete -f myapp.yaml

-----------------------------------------------------------------------------------------------------------------------------

SELECTORS-

Labels:
• key-value pairs used to identify, describe and group related sets of objects or resources.
• Selectors use labels to filter or select objects

COMMANDS-

Deploy the app- kubectl apply -f myapp.yaml
Deploy the service - kubectl apply -f myservice.yaml
To check if the service is connected to the pod (the endpoint will point to the pod IP address) - kubectl get po -o wide
Then get the service endpoint - kubectl get ep myservice
Port forward to the service (Open on browser - http://localhost:8080) - kubectl port-forward service/myservice 8080:80

**If we change the 'app' field inside 'label' in myapp.yaml and keep the 'label' same as before in myservice.yaml
then endpoints will be displayed as none and the port will also not forward to the service. This shows that the
'label' in both the files should be same.

Cleanup - 
kubectl delete -f myservice.yaml
kubectl delete -f myapp.yaml

-----------------------------------------------------------------------------------------------------------------------------

MULTI-CONTAINERS PODS-
• Pods can run one or more containers. One pod is the main container and the others pods provide services to the
main pod.

Patterns-
1. Sidecar: Usinfg this pattern, the helper container provide extra functionality to the main container. If our
app writes some log inside the pod, then sidecar will copy these log files to some purchasing storage offered
provided by the cloud provider. This way the application code inside the main worker is not clutered with the 
infrastructure code. That located in the helper container. If you move from one cloud provider to another one, 
you can simply replace or update the helper code. This keeps the application code clean. 

2. Adapter: Let's say our main worker outputs some complex monitoring information that the monitoring surface 
of the cloud provider cannot understand. The adaptor's role will be to connect to the main worker, simplify the 
data for the monitoring service. Again the code specific to the cloud provider service is located inside helper 
container.

3. Ambassdor: The ambassador pattern of the men of the middle role. Let's say that our application code needs 
to write some nosql database but the code has no clue how to do that. So we can send the data to the ambassador 
that in turn will write the data to the nosql data store. The code specific to the nosql data store service is
located inside the helper container. 

Commands-
Create pod : kubectl create -f [pod-definition.yml]
Exec into a pod : kubectl exec -it [podname] -c [comtainername] -- sh
Get the logs in the container : kubectl logs [podname] -c [containername]

-----------------------------------------------------------------------------------------------------------------------------

NETWORKING CONCEPTS-
• All containers within a pod can communicate with each other.
• All pods can communicate with each other.
• All nodes can communicate with all pods.
• Pods are given an IP address (ephemeral).
• Services are given a persistant IP.

Pod networking-
Inside a cluster network:
• Pods get their own IP address.
• Containers inside a pod share same address space. 
• Containers inside the same pod share same IP address but each must be assigned a different port number.

Inter Pod communication-
• Container inside a pod cannot communicate with a container inside another pod through localhost.
• They need to go through a service to communicate with each other.

External access-
• Traffic goes through a load balancer and service provided by the cloud provider service. 

-----------------------------------------------------------------------------------------------------------------------------

Multi-container pods-

Steps to create:-
Create the pod : kubectl create -f two-containers.yaml
Get some info :
	kubectl get pods -o wide
	kubectl describe pod two-containers
Connect to BusyBox container : kubectl exec -it two-containers --container mybox -- /bin/sh
Fetch the html page served by Nginx container : wget -qO- localhost
Quit : exit
Cleanup : kubectl delete -f two-containers.yaml --force --grace-period=0

-----------------------------------------------------------------------------------------------------------------------------

WORKLOADS -
• A workload is an application running on kubernetes.
• Pod (It is the atomic workload) : Represents the set of containers
• ReplicaSet ----------->(Provides extra functionality on top of pod, for eg, how many instances of pod we want)
• Deployment ______|
• StatefulSet ---------->(These are specialized work loads)
• DeamonSet   _______|    : Provide node-local facilities, such as storage driver or network plugin
• Task that run to completion
	• Job
	• CronJob

###################################################

REPLICASETS -
• Primary method of managing pod replicas and their lifecycle to provide self-healing capabilities
• Their job is to always ensure the desired number of pods are running
• While you can create ReplicaSets, the recommended way is to create Deployments

Self-Healing:- If for some reason one pod crashes, kubernetes will automatically replace it with another pod.
without any human intervention.

It Deployment instead of ReplicaSets as they provide extra functionalities.

###################################################

kubectl - ReplicaSets Commands:-
	• Create a ReplicaSet : kubectl apply -f [definition.yaml]
	• List ReplicaSets : kubectl get rs
	• Get Info : kubectl describe rs [rsName]
	• Delete a ReplicaSet : kubectl delete -f [definition.yaml]
	• Same but using the ReplicaSet name : kubectl delete rs [rsName]
	• Get pods list : kubectl get pods -o wide
	• Cleanup : kubectl delete -f [definition.yaml]

###################################################

Pods vs Deployments-
• Pods don't
	• Self-heal
	• Scale
	• Updates
	• Rollback
• Deployments can

• A deployment manafe a single pod template
• You can create a deployment for each microservice
	• front-end
	• back-end
	• image-processor
	• credit card-processor 
• Deployments can create ReplicaSets in the background
• Don't interact with the ReplicaSets directly

• In short- ReplicaSets provide Self-healing and Scalability 
	functionalities whereas Deployments provide Rolling and
	rollbacks functionalities.

• replicas
	• Number of pod instances
• revisionHistoryLimit
	• Number of previous iterations to keep
• strategy
	• RollingUpdate
		• Cycle through updating pods
	• Recreate
		• All existing pods are killed before new ones are created

###################################################

kubectl - Deployments Commands:-

	• The imperative wey : Kubectl create deploy [deployName] --image=busybox --replicas=3 --port=80
	• Create a deployment : kubectl apply -f [definition.yaml]
	• List Deployments : kubectl get deploy
	• Get info of deployment: kubectl describe deploy [deploymentName]
	• Get info of pod: kubectl describe pod [deploymentName]
	• List replicas : kubectl get rs
	• Describe replicas : kubectl describe rs
	• Delete a deployment : kubectl delete -f [definition.yaml]
	• Same but using the deployment name : kubectl delete deploy [deployName]

###################################################

DaemonSet-
• Ensures all Nodes (or a subset) run an instance of a Pod
• Schedule by the scheduler controller and run by the 
	daemon controller
• As nodes are added to the cluster, Pods are added to them.
• Typical uses
	• Running a cluster storage daemon
	• Running a logs collection on every node
	• Running a node monitoring daemon on every node

###################################################

kubectl- DaemonSets Commands:-

	• Create a DaemonSet : kubectl apply -f [deinition.yaml]
	• List DaemonSets : kubectl get ds
	• Get info : kubectl describe ds [rsName]
	• Delete a DaemonSet : kubectl delete -f [definition.yaml]
	• Same but using the DaemonSet name : kubectl delete ds [rsName]

###################################################

StatefulSet-
• For Pods that must persist or maintain state.
• Unlike a Deployment, a StatefulSet maintains a sticky identity for
	each of their Pods.
• Each has a persistent identifier (name-x)
• If a pod dies, it is replaces with another one using the identifier.
• Creates a series of pods in sequence from 0 to X and deletes them from
	X to 0.
• Typical uses
	• Stabel, unique network identifiers
	• Stable, database using persistent storage.

###################################################

StatefulSets commands-
	• Create a StatefulSet : kubectl apply -f [definition.yaml]
	• List PersistentVolumes claimes : kubectl get pvc
	• List StatefulSet : kubectl get sts
	• Get info : kubectl describe sts [rsName]
	• Delete a StatefulSet : kubectl delete -f [definition.yaml]
	• Same but using the StatefulSet name : kubectl delete sts [rsName]
	
	• Create a file in nginx-sts-2 (Create a session in nginx-sts-2 and create a file in folder mapped to the 
		volume) :
		kubectl exec nginx-sts-2 -it -- /bin/sh
		cd var/www
		echo Hello > hello.txt 

	• Modify default webpage:
		cd /usr/share/nginx/html
		cat > index.html
		Hello
		(Press Ctrl+D)
		cat index.html (To see data inside the file)
		exit

	• Open a session on nginx-sts-0 and reach nginx-sts-2:
		kubectl exec nginx-sts-0  -it -- /bin/sh
		curl http://nginx-sts-2.nginx-headless
		exit
	
	• Delete pod 2:
		kubectl delete pod nginx-sts-2
	
	• Open a session in nginx-sts-2 and see if the file is still present:
		kubectl exec nginx-sts-2 -it -- /bin/sh
		ls var/www
		exit

	• Cleanup:
		kubectl delete -f statefulset.yaml
		kubectl delete pvc www-nginx-sts-0
		kubectl delete pvc www-nginx-sts-1
		kubectl delete pvc www-nginx-sts-2

###################################################

JOB :
	• Workload for short lived tasks.
	• Creates one or mode Pods and ensures that a specific number of them successfully terminate.
	• As pods successfully complete, the Job tracks the successful completions.
	• When a specific number of successful completions is reached, the Job completes.
	• By default, jobs with more than 1 pod will create them one after the other. To create them at the same
		time, add parallelism.
	
Jobs Commands :
	• The inperative way : kubectl create job [jobName] --image=busybox
	• Creates a job : kubectl apply -f [definition.yaml]
	• List jobs : kubectl get jobs
	• Get info : kubectl describe job [jobName]
	• Delete a job : kubectl delete -f [definition.yaml]
	• Same but using Job name : kubectl delete job [jobName]

	• Get the pods list : kubectl logs <podName>

###################################################

CRONJOB :
	• It is workload that runs jobs in a schedule.
	• It is an extension of job.
	• Provides a method of executing jobs on a cron-like schedule.
	• UTC only.

	To check if the job is performed successfully, check history:
		• Last 3 successful jobs are kept.
		• Last failed job is kept.
		• Setting successfulJobHistoryLimit to zero keeps no history.

	CronJobs Commands :
	• The imperative way : kubectl create cronjob [jobName] --image=busybox --schedule="*/1 * * * *" -- bin/sh -c "date;"
	• Create a CronJob : kubectl apply -f [definition.yaml]
	• List CronJobs : kubectl get cj
	• Get info : kubectl describe cj [jobName]
	• Delete a CronJob : kubectl delete -f [definition.yaml]
	• Same but using the CronJob name : kubectl delete cj [jobName]

-----------------------------------------------------------------------------------------------------------------------------

DEPLOYMENTS : 
	• replicas
		• Number of pod instances
	• revisionHistoryLimit
		• Number of previous iterations to keep
	• strategy
		• RollingUpdate
			• Cycle through updating pods
		• Recreate
			• All existing pods are killed before new ones are created

	• RollingUpdate:
		• maxSurge
			• Maximum number of Pods that can be created over the desired number of Pods
			• Value or percentage
		• maxUnavailable
			• Maximum number of Pods that can be unavailable during the update process
		• Default strategy with maxSurge and maxUnavailable both set to 25 %

###################################################

	kubectl - RollingUpdate commands :
		• Update a deployment : kubectl apply -f [definition.yaml]
		• Get the progres of the update : kubectl rollout status
		• Get the history of the deployment : kubectl rollout history deployment [deploymentname]
		• Rollback a deployment : kubectl rollout undo [deploymentname]
		• Rollback to a revision number : kubectl rollout undo [deploymentname] --to-revision [revision#]
		• Cleanup : kubectl delete -f hello-deployment.yaml

###################################################

BLUE-GREEN DEPLOYMENTS :
	If the one of versions have some major changes, for eg, different database schema. Using rollingUpdate
startegy you will have both the versions running at the same time, but this might not work.
So, to remove this problem, blue-green deployments is used. Blue identifies what is in production and green
identifies the newer version deployed but bot yet in production. When everything is ready in the newer version,
update the service definition to point to the new version. So now green will be in production. And green becomes
blue and blue becomes green. 

	Drawbacks-
		• Does not so vet e new ata asesc ema pro em entirely.
		• You need to over provision the cluster size

	Commands :
	1. Deploy
	2. Get pods
	3. Get svc
	4. Port forward and go the localhost on the browser
	5. Deploy second version
	6. Change the app version in the file (here clusterip.yaml)
	7. Update the file : kubectl apply -f clusterip.yaml
	8. Port forward and go the localhost on the browser

-----------------------------------------------------------------------------------------------------------------------------

SERVICES :
	• A service is another type of K8s object
	• Pod IPs are unreliable but service IPs are
	• Durable (unlike pods)
		• Static IP address
		• Static DNS name
		• [servicename].[namespace].svc.cluster.local
	• Services are ways to access pods
	• Target pods using selectors

	Services used in kubernetes :
		• ClusterIP (default)
		• NodePort
		• LoadBalancer L4 (Layer 4)
		• Ingress L7 (Layer 7)
###################################################

	ClusterIP-
		• ClusterlP is the default service
		• Visibility
			• Cluster internal
		• Port
			• The port the service is listening to
		• TargetPort
			• Redirecting to the port the pod(s) are listening to
		• Load balanced using round robin
			• Session affinity is configurable
		• When to use
			• To provide a durable way to communicate with pods inside the cluster
		
	ClusterIP Commands-
		• Create a service to expose a pod : kubectl expose po [podName] --port=80 --target-port=8080 --name=frontend
		• Create a service to expose a deployment : kubectl expose deploy [deployName] --port=80 --target-port=8080
		• Deploy the service : kubectl apply -f [defintion.yaml]
		• Get the services list : kubectl get svc 
		• Get extra info : kubectl get svc -o wide
		• Describe the service : kubectl describe svc [serviceName]
		• Delete the service : kubectl delete -f [definition.yaml]
		• Delete the service using it's name : kubectl delete svc [serviceName]
		
###################################################
	
	NodePort-
		• NodePort extend the ClusterlP service
		• Visibility
			• Internal and external
		• NodePort
			• The port the service is listening to externally
			• Statically defined, or dynamically taken from a range between 30000-32767
		• Port
			• The port the service is listening to internally
		• TargetPort
			• Redirecting to the port the pod(s) are listening to
		• Nodes
			• Must have public IP addresses
		• Use any Node IP + nodePort to access the service

	kubectl - NodePort commands :
		• Create a service to expose a pod : kubectl expose po [podName] --port=80 --target-port=8080 --type=NodePort
		• Create a service to expose a deployment : kubectl expose deploy [deployName] --port=80 --target-port=8080 --type=NodePort --name=frontend
		• Deploy the service : kubectl apply -f [definition.yaml]
		• Get the services list : kubectl get svc
		• Get extra info : kubectl get svc -o wide
		• Describe the service : kubectl describe svc [serviceName]
		• Delete the service : kubectl delete -f [definition.yaml]
		• Delete the service using it's name : kubectl delete svc [serviceName]

###################################################

	Load Balancing :
		• Layer 4 Load Balancing
			• Operating at the transport level (TCP)
			• Unable to make decisions based on content
			• Simple algorithms such as round-robin routing
		• Layer 7 Load Balancing
			• Operates at the application level (HTTP, SMTP, etc)
			• Able to make decisions based on the actual content of each message
			• More intelligent load balancing decisions and content optimizations
			• Routing rules 

-----------------------------------------------------------------------------------------------------------------------------

STORAGE AND PERDISTENCE :

Containers are ephemerous and stateless so we need to find a way to store data outside the containers.
For this purpose, we use volumes.

	Volumes :
		• We need to store data outside the container in a Volume
		• Volumes let containers store data into external storage systems
		• Vendors create plugins for their storage systems according to the Container Storage Interface
		• Two ways to create storage
			• Static and dynamic
	The cloud provider creates plugins to expose their storage services as Persistent Volume and StorageClass.
	These two are kubernetes objects. 

###################################################

	The static way :
		Persistent Volumes and Claims
			• Persistent Volumes
			• Represent a storage resource
			• Cluster wide
			• Provisioned by an administrator
			• Persistent Volume Claim
			• A one-to-one mapping to a persistent volume
			• One or more pods can use a Persistent Volume Claim
			• Can be consumed by any of the containers within the pod

		Types of persistent volumes:
			• GCEPersistentDisk							• Cinder (Open stack block storage)
			• AWSElasticBlockStore						• Glusterfs
			• AzureFile									• VsphereVolumes 
			• AzureDisk									• Quobyte Volumes
			• CSI										• Portworx Volumes
			• FC (Fibre Channel)						• ScaleIO Volumes
			• FlexVolume								• StorageOS
			• Flocker									• HostPath
			• NFS											• Single node testing only - local storage is not supported in any way and will not work on multi-node cluster.
			• iSCSI
			• RBD (ceph Block Device)
			• CephFS

		Reclaim Policies :
			• Delete
				• Delete the data upon pods deletion
				• The default
			• Retain
				• Keeps the data upon pods deletion

	Access Modes :
		• ReadWriteMany
			• The volume can be mounted as read-write by many pods
		• ReadOnlyMany
			• The volume can be mounted read-only by many pods
		• ReadWriteOnce
			• The volume can be mounted as read-write by a single pod
			• The other pods are in read-only mode
			• The one that has mounted the volume first will be able to write
	
	Persistent Volume States :
		• Available
			• Afree resource that is not yet bound to a claim
		• Bound
			• The volume is bound to a claim
		• Released
			• The claim has been deleted, but the resource is not yet reclaimed by the cluster
		• Failed
			• The volume has failed its automatic reclamation
	
	kubectl PV and PVC Commands :
		• Deploy the PVs and PVCs : kubectl apply -f [defintion.yaml]
		• Get the PV list : kubectl get pv
		• Get the PVC list : kubectl get pvc
		• Describe the PV : kubectl describe pv [pvName]
		• Describe the PVC : kubectl describe pvc [pvcName]
		• Delete the PVs and PVCs : kubectl delete -f [defintion.yaml]
		• Delete the pv using it's name : kubectl delete pv [pvName]
		• Delete the pvc using it's name : kubectl delete pvc [pvcName]
		• Delete pod : kubectl delete -f pod.yaml --force --grace-period=0

###################################################
	 
	 Dynamic way :
	 	StorageClass-
			• Describes the "classes" of storage offered by the admin
			• An abstraction on top of an external storage resource
			• No need to set a capacity
			• Eliminates the need for the admin to pre-provision a persistent volume

		Reclaim Policies
			• Delete
				• Delete the data upon pods deletion
				• The default
			• Retain
				• Keeps the data upon pods deletion
		
		Access Modes
			• ReadWriteMany
				• The volume can be mounted as read-write by many pods
			• ReadOnlyMany
				• The volume can be mounted read-only by many pods
			• ReadWriteOnce
				• The volume can be mounted as read-write by a single pod
				• The other pods are in read-only mode
				• The one that has mounted the volume first will be able to write
		
		StorageClass Commands -
			• Deploy the StorageClass or PVC : kubectl apply -f [definition.yaml]
			• Get the StorageClass list : kubectl get sc 
			• Get the PVC list : kubectl get pvc
			• Describe the StorageClass : kubectl describe sc [className]
			• Delete the SC and PVC : kubectl delete -f [definition.yaml]
			• Delete the SC using it's name : kubectl delete sc [className]
			• Delete the PVC using it's name : kubectl delete pvc [pvcName]

-----------------------------------------------------------------------------------------------------------------------------

ConfigMaps :
	• Decouple and externalize configuration
	• Referenced as environment variables
	• Created from
		• Manifests
		• Files
		• Directories (containing one or more file)
	• Static meaning that if you change values, the containers will have to be restarted to get them

	ConfigMaps and Volumes
		• Map Volumes on ConfigMaps
		• Solve the static issue
		• Updates to values are reflected in containers
		• Each Key/Value pair is seen as a file in the mounted directory

	kubectl - ConfigMaps Commands :
		• The imperative way : kubectl create configmap literal-example --from-literal="city=Ann Arbor" --from-literal=state=Michigan
		• The declarative way : kubectl apply -f [cf.yaml]
		• From a file : kubectl create cm [name] --from-file=myconfig.txt
		• From a folder : kubectl create cm [name] --from-file=myconfig/
		• List the ConfigMaps : kubectl create cm 
		• Save a ConfigMap in a YAML file : kubectl create cm [name] -o YAML
		• Delete a ConfigMap : kubectl delete -f [cf.yaml]
		• Delete pod : kubectl delete -f pod.yaml --grace-period=0 --force

		• Get ConfigMap info : kubectl describe configmap cm-example
		• Get ConfigMap info (same output in YAML format) : kubectl get configmap cm-example -d YAML

###################################################

	Secrets :

		BUILTIN TYPE								USAGE
		Opaque										arbitrary user-defined data
		kubernetes.io/service-account-token			serivce account token
		kubernetes.io/dockercfg						serialized ~/.dockercfg file
		kubernetes.io/dockerconfigjson				serialized ~/.docker/config.json file
		kubernetes.io/basic-auth					credentials for basic authentication
		kubernetes.io/ssh-auth						credentials for SSH authentication
		kubernetes.io/tls							data for a TLS client or server
		bootstrap.kubernetes.io/token				bootstrap token data
	
	• Stored as base64 encoded strings
	• Not secure as base64 strings are not encrypted
	• Protect secrets using RBAC authorization policies
	• Store secrets elsewhere
		• Cloud providers offer ways to secure these secrets
			• Azure Key Vault
			• AWS Key Management Service
			• Google Cloud KMS
		• HarshiCorp Vault
			• https://www.vaultproject.io/
		
	kubectl - Secrets Commands :
		•The imperative way : kubectl create secret generic [secretName] --from-literal=STATE=Michigan
		• The declarative way : kubectl apply -f [secret.yaml]
		• List the Secrets : kubectl get secrets
		• Describe secrets : kubectl describe secret secrets
		• Save a Secret in a YAML file : kubectl get secrets [secretName] -o YAML
		• Delete a secret : kubectl delete -f [secret.yaml]
		• Delete a secret : kubectl delete secrets [secretName]

-----------------------------------------------------------------------------------------------------------------------------

OBSERVABILITY :
	Observability - Probes
		• Startup probes
			• To know when a container has started
		• Readiness probes
			• To know when a container is ready to accept traffic
			• A failing readiness probe will stop the application from receiving traffic
		• Liveness probes
			• Indicates whether the code is running or not
			• A failing liveness probe will restart the container

	Probing the container
		• The kubelet checks periodically the containers using probes
		• ExecAction
			• Execute a command inside the container
		• TCPSockectAction
			• Check if a TCP socket port is open
		• HTTPGetAction
			• Performs an HTTP GET against a specific port and path

-----------------------------------------------------------------------------------------------------------------------------

 DASHBOARDS :
	K8s dashboard - WebUI :
		• Kubernetes offers a Web Ul
		• Not installed by default by
			• Docker Desktop
			• AWS EKS
			• Azure AKS
			• GCPGKE
			• Linode LKE
		• Don't install it if not needed

	Lens
		• Kubernetes IDE
		• Runs locally
			• Mac, Windows, Linux
		• Built-in terminal
		• Maintained by Mirantis
			• https://k8slens.dev

	Dashboard — K9s
		• Dashboard in a terminal!
		• Windows
			• choco install k9s
		• Mac
			• brew install k9s
		• Linux
			• https://github.com/derailed/k9s

-----------------------------------------------------------------------------------------------------------------------------

SCALING PODS :

	Horizontal Pod AutoScaling :
		• Uses the K8s Metrics Server
		• Pods must have requests and limits defined
		• The HPA checks the Metrics Server every 30 seconds
		• Scale according to the min and max number or replicas defined
		• Cooldown / Delay
			• Prevent racing conditions
			• Once a change has been made, HPA waits
			• By default, the delay on scale up events is 3 minutes, and the delay on scale down events is 5 minutes

	HPA Commands:
		• The imperative way : kubectl autoscale deployment [name] --cpu-percent=50 --min=3 --max=10
		• The declarative way : kubectl apply -f [hap.yaml]
		• Get the autoscaler status/Validate : kubectl get hpa [name] 
		• Delete the HPA : kubectl delete -f [hap.name]
		• Delete the HPA : kubectl delete hpa [name]

		To check if Metrics Server is installed in the system, we will look for a pod called metrics-server
		in the kube-systems namespace : kubectl get po -n kube-system 
	
		To download metrics server : kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml
		Autoscaling limits : kubectl autoscale deployment hpa-deployment --cpu-percent=50 --min=1 --max=4
